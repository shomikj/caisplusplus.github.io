---
layout: post
published: true
title: "k-Nearest Neighbors"
mathjax: true
featured: true
comments: true
---
<h2><a name="intro"></a>k-NN: k-Nearest Neighbors</h2>

k-Nearest Neighbors (k-NN) is one of the simplest machine learning algorithms and can be used for both classification and regression problems. To illustrate how the k-NN algorithm works, let's consider a simple example of classifying points in the 2D plane into either a red or blue class. As shown in the figure below, our task is to classify the green point $$(4, 5)$$.

<img src="/images/knn1.png" style="width: 50%;"/>

The crux of the k-NN algorithm is to examine the $$k$$ closest training examples to our test element, where $$k$$ is a chosen hyperparameter. We measure closeness using feature similarity: how similar two elements are on the basis of their features.

In this example, our features are just the coordinates of our points. Recall that Euclidean distance between two points $$p = (p_{1}, p_{2},...,p_{n})$$ and $$q = (q_{1}, q_{2},...,q_{n})$$ is given by:

$$d(p,q) = d(q,p) = \sqrt{(p_{1} - q_{1})^2 + (p_{2} - q_{2})^2 + ... + (p_{n} - q_{n})^2}$$

Therefore, we can use Euclidean distance to find the $$k$$ closest points to the green test point. We will use $$k=3$$ in this example.

<img src="/images/knn2.png" style="width: 50%;"/>

For classification, the k-NN algorithm outputs the class most common among its $$k$$ nearest neighbors. Thus, because 2 of the 3 neighboring points we examined are red, we can classify our test point as red as well.

<img src="/images/knn3.png" style="width: 50%;"/>

Beyond our simple example, for regression, k-NN outputs the average of the values among its $$k$$ nearest neighbors. To improve performance in cases where there is a skewed proportion of training examples per class, the distance of each neighbor is often weighted into the decision process. In addition, if features are not continuous variables, an overlap metric such as the [**Hamming Distance**](https://en.wikipedia.org/wiki/Hamming_distance) can be used. Despite its simplicity, k-NN often works surprisingly well for many real-life examples.

<h2><a name="review"></a>Ben's Review/Challenge Questions</h2>




<h2>Sources</h2>
<ul>
  <li><a href='https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Statistical_setting'>wikipedia.org/K-nearest_neighbors</a></li>
  <li><a href='https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7'>medium.com/a-quick-introduction-to-k-nearest-neighbors-algorithm</a></li>
  <li><a href='https://en.wikipedia.org/wiki/Hamming_distance'>wikipedia.org/Hamming_distance</a></li>
  <li><a href='https://en.wikipedia.org/wiki/Euclidean_distance'>wikipedia.org/Euclidean_distance</a></li>
  <li><a href='https://www.desmos.com/'>desmos.com</a></li>
</ul>
